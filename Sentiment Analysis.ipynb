{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import resample,shuffle  # Import resample function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# distinguishes stopwords \n",
    "# implements the punkt sentence tokenizer algorithm to break up sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2401  Borderlands  Positive  \\\n",
      "0    2401  Borderlands  Positive   \n",
      "1    2401  Borderlands  Positive   \n",
      "2    2401  Borderlands  Positive   \n",
      "3    2401  Borderlands  Positive   \n",
      "4    2401  Borderlands  Positive   \n",
      "..    ...          ...       ...   \n",
      "995  2577  Borderlands  Positive   \n",
      "996  2577  Borderlands  Positive   \n",
      "997  2577  Borderlands  Positive   \n",
      "998  2577  Borderlands  Positive   \n",
      "999  2577  Borderlands  Positive   \n",
      "\n",
      "    im getting on borderlands and i will murder you all ,  \n",
      "0    I am coming to the borders and I will kill you...     \n",
      "1    im getting on borderlands and i will kill you ...     \n",
      "2    im coming on borderlands and i will murder you...     \n",
      "3    im getting on borderlands 2 and i will murder ...     \n",
      "4    im getting into borderlands and i can murder y...     \n",
      "..                                                 ...     \n",
      "995              Who's down for some @Borderlands on       \n",
      "996                    Who's on for some @ Borderlands     \n",
      "997                             Who's at @ Borderlands     \n",
      "998               Who's down with some @Borderlands on     \n",
      "999      Who't s someone down for some @Borderlands on     \n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "num_rows_to_load = 1000\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\maadh\\\\Downloads\\\\twitter_training.csv.zip\", nrows = num_rows_to_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2401', 'Borderlands', 'Positive',\n",
       "       'im getting on borderlands and i will murder you all ,'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'im getting on borderlands and i will murder you all ,': 'text'}, inplace=True)\n",
    "df.rename(columns={'Positive': 'sentiment'}, inplace=True)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error was that The first letters of each sentiment weren't capitalized\n",
    "sentiment_mapping = {'Negative': 0, 'Positive': 1, 'Neutral': 2}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sentiment values before cleanup: [ 1.  2.  0. nan]\n",
      "Unique sentiment values after cleanup: [1. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique sentiment values before cleanup:\", df['sentiment'].unique())\n",
    "\n",
    "# Remove rows with invalid labels\n",
    "valid_labels = [0, 1, 2]\n",
    "df = df[df['sentiment'].isin(valid_labels)]\n",
    "\n",
    "print(\"Unique sentiment values after cleanup:\", df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "1.0    423\n",
      "2.0    279\n",
      "0.0    192\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Balance the dataset if necessary\n",
    "df_negative = df[df['sentiment'] == 0]\n",
    "df_positive = df[df['sentiment'] == 1]\n",
    "df_neutral = df[df['sentiment'] == 2]\n",
    "\n",
    "df_positive_resampled = resample(df_positive, replace=True, n_samples=len(df_negative), random_state=42)\n",
    "df_neutral_resampled = resample(df_neutral, replace=True, n_samples=len(df_negative), random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_negative, df_positive_resampled, df_neutral_resampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = shuffle(df_balanced, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['sentiment'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, padding='post', maxlen=256)\n",
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='post', maxlen=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=16, input_length=256),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Change loss function for multi-class classification\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 289ms/step - loss: 1.0982 - accuracy: 0.3455 - val_loss: 1.0955 - val_accuracy: 0.5084\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0944 - accuracy: 0.4657 - val_loss: 1.0909 - val_accuracy: 0.5084\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.0923 - accuracy: 0.4643 - val_loss: 1.0875 - val_accuracy: 0.5084\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0896 - accuracy: 0.4643 - val_loss: 1.0839 - val_accuracy: 0.5084\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.0868 - accuracy: 0.4643 - val_loss: 1.0802 - val_accuracy: 0.5084\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.0850 - accuracy: 0.4643 - val_loss: 1.0764 - val_accuracy: 0.5084\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0827 - accuracy: 0.4643 - val_loss: 1.0725 - val_accuracy: 0.5084\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0792 - accuracy: 0.4643 - val_loss: 1.0684 - val_accuracy: 0.5084\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0769 - accuracy: 0.4643 - val_loss: 1.0643 - val_accuracy: 0.5084\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.0749 - accuracy: 0.4643 - val_loss: 1.0602 - val_accuracy: 0.5084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels), batch_size=512, verbose=1)\n",
    "#input values aren't normalized which is causing error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0602 - accuracy: 0.5084\n",
      "Test Accuracy: 0.5083798766136169\n",
      "6/6 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_padded, test_labels)\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_padded)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text Predicted Sentiment  \\\n",
      "799      really like randy pitchford , 's helped lot .            Positive   \n",
      "489  tales behind borderlands swaggiedeals . com / ...            Positive   \n",
      "606  gamespot : borderlands 3 's dlc rejects major ...            Positive   \n",
      "816  went bed 4am . 5 hours earlier 9pm graveward f...            Positive   \n",
      "39   man gearbox really needs fix dissapointing dro...            Positive   \n",
      "\n",
      "    Actual Sentiment  \n",
      "799          Neutral  \n",
      "489          Neutral  \n",
      "606          Neutral  \n",
      "816         Negative  \n",
      "39          Negative  \n"
     ]
    }
   ],
   "source": [
    "reverse_sentiment_mapping = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n",
    "\n",
    "# Convert numeric predictions to sentiment strings\n",
    "predicted_sentiments = [reverse_sentiment_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# Print or save the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': test_data,\n",
    "    'Predicted Sentiment': predicted_sentiments,\n",
    "    'Actual Sentiment': test_labels.map(reverse_sentiment_mapping)\n",
    "})\n",
    "\n",
    "print(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
